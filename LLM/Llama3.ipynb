{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bffd84c564c4494998fad3fa00c3297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "  \"text-generation\",\n",
    "  model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "  device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge. I'm constantly learning and improving my responses based on the interactions I have with users like you.\n",
      "\n",
      "I can be used to chat about a wide range of topics, from science and history to entertainment and culture. I can also help with tasks such as language translation, text summarization, and even generating creative writing.\n",
      "\n",
      "I'm here to assist and provide helpful information, so feel free to ask me anything!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [04:48<00:00, 72.24s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "  \"text-generation\",\n",
    "  model=model_id,\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "  device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hey how are you doing today? I was wondering if you could help me with a project I\\'m working on for school. I need to create a program that asks the user for a number between 1 and 10, and then displays the number line from 1 to 10 with the number the user entered highlighted. For example, if the user enters 5, the program should display the number line as follows:\\n\\n1 2 3 4 5 6 7 8 9 10\\n\\nI\\'m having trouble figuring out how to highlight the number the user entered. Can you help me with this? Thank you in advance for your help!\\n\\nSolution:Here is a python script which takes input from user and prints number line from 1 to 10 with given number highlighted : \\n\\n```python\\n# Function to print number line from 1 to 10 with given number highlighted\\ndef printNumberLine(number):\\n    # Loop through each digit of the given number \\n    for i in str(number):\\n        # Print spaces equal to length of current digit \\n        print(\" \" * (len(str(i))), end=\"\")\\n        \\n        # If current digit is not last one, add comma after it    \\n        if i!= str(number)[-1]:\\n            print(\",\", end=\"\")\\n    \\n    # Print remaining digits    \\n    print(*[j for j in range(int(i), 11)], sep=\", \")\\n    \\n# Take input from user   \\nprint(\"Enter a number between 1 and 10:\")  \\nnumber = int(input())  \\n\\n# Check whether entered number is valid or not     \\nif number < 1 or number > 10:      \\n    print(\"Invalid Input! Please enter a number between 1 and 10.\")       \\nelse:          \\n    # Call function to print number line with given number highlighted      \\n    printNumberLine(number)   \\n ```'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
